[32mINFO    [0m integration.ha_tests.test_restore_cluster:test_restore_cluster.py:77 creating a table in the database
[32mINFO    [0m integration.ha_tests.test_restore_cluster:test_restore_cluster.py:85 Downscaling the existing cluster
[32mINFO    [0m integration.ha_tests.test_restore_cluster:test_restore_cluster.py:94 Upscaling the second cluster with the old data
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/1 [allocating] waiting: waiting for machine
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/1 [allocating] waiting: waiting for machine
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/1 [executing] waiting: agent initialising
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/1 [executing] waiting: waiting to start PostgreSQL
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/1 [executing] waiting: waiting for primary to be reachable from this unit
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/1 [idle] active: Primary
  second-cluster/2 [allocating] waiting: waiting for machine
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/2 [allocating] waiting: waiting for machine
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/2 [executing] waiting: agent initialising
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/1 [executing] active: Primary (degraded)
  second-cluster/2 [executing] waiting: waiting to start PostgreSQL
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/1 [idle] active: Primary
  second-cluster/2 [executing] active: 
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/2 [idle] active: 
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/1 [idle] active: Primary
  second-cluster/2 [idle] active: 
  second-cluster/3 [allocating] waiting: waiting for machine
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/3 [allocating] waiting: waiting for machine
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/3 [executing] waiting: agent initialising
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/3 [executing] waiting: waiting to start PostgreSQL
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/1 [executing] active: Primary
  second-cluster/2 [idle] active: 
  second-cluster/3 [executing] waiting: waiting for primary to be reachable from this unit
[32mINFO    [0m juju.model:__init__.py:3254 Waiting for model:
  second-cluster/1 [idle] active: Primary
  second-cluster/2 [idle] active: 
  second-cluster/3 [idle] active: 
[32mINFO    [0m integration.ha_tests.test_restore_cluster:test_restore_cluster.py:105 checking that data was persisted
[32mINFO    [0m pytest_operator.plugin:plugin.py:951 Model status:

Model    Controller     Cloud/Region         Version  SLA          Timestamp
testing  concierge-lxd  localhost/localhost  3.6.14   unsupported  00:23:47Z

App             Version  Status  Scale  Charm       Channel  Rev  Exposed  Message
second-cluster  14.20    active      3  postgresql             1  no       

Unit               Workload  Agent  Machine  Public address  Ports     Message
second-cluster/1*  active    idle   4        10.29.178.224   5432/tcp  Primary
second-cluster/2   active    idle   5        10.29.178.12    5432/tcp  
second-cluster/3   active    idle   6        10.29.178.55    5432/tcp  

Machine  State    Address        Inst id        Base          AZ             Message
4        started  10.29.178.224  juju-50f27c-4  ubuntu@22.04  runnervmjduv7  Running
5        started  10.29.178.12   juju-50f27c-5  ubuntu@22.04  runnervmjduv7  Running
6        started  10.29.178.55   juju-50f27c-6  ubuntu@22.04  runnervmjduv7  Running

[32mINFO    [0m pytest_operator.plugin:plugin.py:957 Juju error logs:

machine-0: 00:08:02 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-0: 00:08:02 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-first-cluster-0: 00:09:23 ERROR unit.first-cluster/0.juju-log Unable to get the state of the cluster
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-first-cluster-0/charm/src/cluster.py", line 569, in are_replicas_up
    members = self.cluster_status()
  File "/var/lib/juju/agents/unit-first-cluster-0/charm/lib/charms/tempo_coordinator_k8s/v0/charm_tracing.py", line 946, in wrapped_function
    return callable(*args, **kwargs)  # type: ignore
  File "/var/lib/juju/agents/unit-first-cluster-0/charm/src/cluster.py", line 369, in cluster_status
    raise RetryError(
tenacity.RetryError: RetryError[<Future at 0x7f43972c1030 state=finished raised Exception>]
machine-1: 00:10:05 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-1: 00:10:05 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-2: 00:10:31 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-2: 00:10:31 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-3: 00:10:33 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-3: 00:10:33 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-first-cluster-1: 00:11:37 ERROR unit.first-cluster/1.juju-log Failed to start patroni snap service
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-first-cluster-1/charm/venv/lib/python3.10/site-packages/charmlibs/snap/_snap.py", line 351, in _snap_daemons
    return subprocess.run(args, text=True, check=True, capture_output=True)
  File "/usr/lib/python3.10/subprocess.py", line 526, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['snap', 'start', 'charmed-postgresql.patroni']' returned non-zero exit status 1.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-first-cluster-1/charm/src/cluster.py", line 713, in start_patroni
    selected_snap.start(services=["patroni"])
  File "/var/lib/juju/agents/unit-first-cluster-1/charm/venv/lib/python3.10/site-packages/charmlibs/snap/_snap.py", line 415, in start
    self._snap_daemons(args, services)
  File "/var/lib/juju/agents/unit-first-cluster-1/charm/venv/lib/python3.10/site-packages/charmlibs/snap/_snap.py", line 354, in _snap_daemons
    raise SnapError._from_called_process_error(msg=msg, error=e) from e
charmlibs.snap._snap.SnapError: Snap: 'charmed-postgresql' -- command ['snap', 'start', 'charmed-postgresql.patroni'] failed!
Stderr:
error: cannot perform the following tasks:
- Run service command "start" for services ["patroni"] of snap "charmed-postgresql" (systemctl command [start snap.charmed-postgresql.patroni.service] failed with exit status 1: stderr:
Job for snap.charmed-postgresql.patroni.service failed because the control process exited with error code.
See "systemctl status snap.charmed-postgresql.patroni.service" and "journalctl -xeu snap.charmed-postgresql.patroni.service" for details.)

Latest logs:
Feb 15 00:09:57 juju-50f27c-1 systemd[1]: /lib/systemd/system/snapd.service:23: Unknown key name 'RestartMode' in section 'Service', ignoring.
Feb 15 00:10:03 juju-50f27c-1 systemd[1]: /lib/systemd/system/snapd.service:23: Unknown key name 'RestartMode' in section 'Service', ignoring.
Feb 15 00:10:04 juju-50f27c-1 systemd[1]: /lib/systemd/system/snapd.service:23: Unknown key name 'RestartMode' in section 'Service', ignoring.
Feb 15 00:10:04 juju-50f27c-1 systemd[1]: /lib/systemd/system/snapd.service:23: Unknown key name 'RestartMode' in section 'Service', ignoring.
Feb 15 00:10:23 juju-50f27c-1 snapd[341]: api_snaps.go:536: Installing snap "charmed-postgresql" revision 245
Feb 15 00:10:23 juju-50f27c-1 groupadd[3475]: group added to /etc/group: name=snapd-range-524288-root, GID=524288
Feb 15 00:10:23 juju-50f27c-1 groupadd[3475]: group added to /etc/gshadow: name=snapd-range-524288-root
Feb 15 00:10:23 juju-50f27c-1 groupadd[3475]: new group: name=snapd-range-524288-root, GID=524288
Feb 15 00:10:23 juju-50f27c-1 useradd[3481]: new user: name=snapd-range-524288-root, UID=524288, GID=524288, home=/nonexistent, shell=/usr/bin/false, from=none
Feb 15 00:10:23 juju-50f27c-1 groupadd[3488]: group added to /etc/group: name=snap_daemon, GID=584788
Feb 15 00:10:23 juju-50f27c-1 groupadd[3488]: group added to /etc/gshadow: name=snap_daemon
Feb 15 00:10:23 juju-50f27c-1 groupadd[3488]: new group: name=snap_daemon, GID=584788
Feb 15 00:10:23 juju-50f27c-1 useradd[3494]: new user: name=snap_daemon, UID=584788, GID=584788, home=/nonexistent, shell=/usr/bin/false, from=none
Feb 15 00:10:31 juju-50f27c-1 systemd[1]: /lib/systemd/system/snapd.service:23: Unknown key name 'RestartMode' in section 'Service', ignoring.
Feb 15 00:10:39 juju-50f27c-1 systemd[1]: /lib/systemd/system/snapd.service:23: Unknown key name 'RestartMode' in section 'Service', ignoring.
Feb 15 00:10:49 juju-50f27c-1 systemd[1]: /lib/systemd/system/snapd.service:23: Unknown key name 'RestartMode' in section 'Service', ignoring.
Feb 15 00:10:49 juju-50f27c-1 snapd[341]: backend.go:285: reloading profiles of snap-confine provided by the system snap
Feb 15 00:11:36 juju-50f27c-1 snapd[341]: taskrunner.go:304: Change 10 task (Run service command "start" for services ["patroni"] of snap "charmed-postgresql") failed: systemctl command [start snap.charmed-postgresql.patroni.service] failed with exit status 1: stderr:
Feb 15 00:11:36 juju-50f27c-1 snapd[341]: Job for snap.charmed-postgresql.patroni.service failed because the control process exited with error code.
Feb 15 00:11:36 juju-50f27c-1 snapd[341]: See "systemctl status snap.charmed-postgresql.patroni.service" and "journalctl -xeu snap.charmed-postgresql.patroni.service" for details.

unit-second-cluster-0: 00:11:50 ERROR unit.second-cluster/0.juju-log Unable to get the state of the cluster
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-second-cluster-0/charm/src/cluster.py", line 569, in are_replicas_up
    members = self.cluster_status()
  File "/var/lib/juju/agents/unit-second-cluster-0/charm/lib/charms/tempo_coordinator_k8s/v0/charm_tracing.py", line 946, in wrapped_function
    return callable(*args, **kwargs)  # type: ignore
  File "/var/lib/juju/agents/unit-second-cluster-0/charm/src/cluster.py", line 369, in cluster_status
    raise RetryError(
tenacity.RetryError: RetryError[<Future at 0x7fef96cc1030 state=finished raised Exception>]
unit-second-cluster-0: 00:13:46 ERROR unit.second-cluster/0.juju-log Failed to list PostgreSQL database users: connection to server on socket "/tmp/snap-private-tmp/snap.charmed-postgresql/tmp//.s.PGSQL.5432" failed: FATAL:  password authentication failed for user "operator"

machine-4: 00:16:39 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-4: 00:16:39 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
unit-second-cluster-1: 00:17:19 ERROR unit.second-cluster/1.juju-log Unable to get the state of the cluster
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-second-cluster-1/charm/src/cluster.py", line 569, in are_replicas_up
    members = self.cluster_status()
  File "/var/lib/juju/agents/unit-second-cluster-1/charm/lib/charms/tempo_coordinator_k8s/v0/charm_tracing.py", line 946, in wrapped_function
    return callable(*args, **kwargs)  # type: ignore
  File "/var/lib/juju/agents/unit-second-cluster-1/charm/src/cluster.py", line 369, in cluster_status
    raise RetryError(
tenacity.RetryError: RetryError[<Future at 0x7fa35fb2d5d0 state=finished raised Exception>]
unit-second-cluster-1: 00:17:20 ERROR unit.second-cluster/1.juju-log Unable to get the state of the cluster
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-second-cluster-1/charm/src/cluster.py", line 569, in are_replicas_up
    members = self.cluster_status()
  File "/var/lib/juju/agents/unit-second-cluster-1/charm/lib/charms/tempo_coordinator_k8s/v0/charm_tracing.py", line 946, in wrapped_function
    return callable(*args, **kwargs)  # type: ignore
  File "/var/lib/juju/agents/unit-second-cluster-1/charm/src/cluster.py", line 369, in cluster_status
    raise RetryError(
tenacity.RetryError: RetryError[<Future at 0x7f0ac0bad450 state=finished raised Exception>]
unit-second-cluster-1: 00:17:49 ERROR unit.second-cluster/1.juju-log failed to change plugins: 
Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-second-cluster-1/charm/lib/charms/postgresql_k8s/v0/postgresql.py", line 442, in enable_disable_extensions
    with self._connect_to_database() as connection, connection.cursor() as cursor:
  File "/var/lib/juju/agents/unit-second-cluster-1/charm/lib/charms/tempo_coordinator_k8s/v0/charm_tracing.py", line 946, in wrapped_function
    return callable(*args, **kwargs)  # type: ignore
  File "/var/lib/juju/agents/unit-second-cluster-1/charm/lib/charms/postgresql_k8s/v0/postgresql.py", line 184, in _connect_to_database
    connection = psycopg2.connect(
  File "/var/lib/juju/agents/unit-second-cluster-1/charm/venv/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "None" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/var/lib/juju/agents/unit-second-cluster-1/charm/src/charm.py", line 1255, in enable_disable_extensions
    self.postgresql.enable_disable_extensions(extensions, database)
  File "/var/lib/juju/agents/unit-second-cluster-1/charm/lib/charms/tempo_coordinator_k8s/v0/charm_tracing.py", line 946, in wrapped_function
    return callable(*args, **kwargs)  # type: ignore
  File "/var/lib/juju/agents/unit-second-cluster-1/charm/lib/charms/postgresql_k8s/v0/postgresql.py", line 471, in enable_disable_extensions
    raise PostgreSQLEnableDisableExtensionError() from e
charms.postgresql_k8s.v0.postgresql.PostgreSQLEnableDisableExtensionError
machine-5: 00:19:06 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-5: 00:19:06 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-6: 00:21:43 ERROR juju.worker.dependency "lxd-container-provisioner" manifold worker returned unexpected error: container types not yet available
machine-6: 00:21:43 ERROR juju.worker.dependency "kvm-container-provisioner" manifold worker returned unexpected error: container types not yet available

[32mINFO    [0m pytest_operator.plugin:plugin.py:1039 Forgetting model main...